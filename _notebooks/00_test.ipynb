{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c5f8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 1.11.1 of Apache Avro\r\n",
      "Copyright 2010-2015 The Apache Software Foundation\r\n",
      "\r\n",
      "This product includes software developed at\r\n",
      "The Apache Software Foundation (https://www.apache.org/).\r\n",
      "----------------\r\n",
      "Available tools:\r\n",
      "    canonical  Converts an Avro Schema to its canonical form\r\n",
      "          cat  Extracts samples from files\r\n",
      "      compile  Generates Java code for the given schema.\r\n",
      "       concat  Concatenates avro files without re-compressing.\r\n",
      "        count  Counts the records in avro files or folders\r\n",
      "  fingerprint  Returns the fingerprint for the schemas.\r\n",
      "   fragtojson  Renders a binary-encoded Avro datum as JSON.\r\n",
      "     fromjson  Reads JSON records and writes an Avro data file.\r\n",
      "     fromtext  Imports a text file into an avro data file.\r\n",
      "      getmeta  Prints out the metadata of an Avro data file.\r\n",
      "    getschema  Prints out schema of an Avro data file.\r\n",
      "          idl  Generates a JSON schema from an Avro IDL file\r\n",
      " idl2schemata  Extract JSON schemata of the types from an Avro IDL file\r\n",
      "       induce  Induce schema/protocol from Java class/interface via reflection.\r\n",
      "   jsontofrag  Renders a JSON-encoded Avro datum as binary.\r\n",
      "       random  Creates a file with randomly generated instances of a schema.\r\n",
      "      recodec  Alters the codec of a data file.\r\n",
      "       repair  Recovers data from a corrupt Avro Data file\r\n",
      "  rpcprotocol  Output the protocol of a RPC service\r\n",
      "   rpcreceive  Opens an RPC Server and listens for one message.\r\n",
      "      rpcsend  Sends a single RPC message.\r\n",
      "       tether  Run a tethered mapreduce job.\r\n",
      "       tojson  Dumps an Avro data file as JSON, record per line or pretty.\r\n",
      "       totext  Converts an Avro data file to a text file.\r\n",
      "     totrevni  Converts an Avro data file to a Trevni file.\r\n",
      "  trevni_meta  Dumps a Trevni file's metadata as JSON.\r\n",
      "trevni_random  Create a Trevni file filled with random instances of a schema.\r\n",
      "trevni_tojson  Dumps a Trevni file as JSON.\r\n"
     ]
    }
   ],
   "source": [
    "# first of all, let's check that avro tools work\n",
    "\n",
    "!avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea2dc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## HIVE WORKS! Existing schemas: [('default',)] ##\n"
     ]
    }
   ],
   "source": [
    "from pyhive import hive\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "# A simple loop waiting for the Hive to become available\n",
    "while True:\n",
    "    try:\n",
    "        h = hive.Connection(host='hiveserver2', port=10000).cursor()\n",
    "        h.execute('SHOW SCHEMAS')\n",
    "        print(\"\")\n",
    "        print(\"## HIVE WORKS! Existing schemas: {} ##\".format(h.fetchall()))\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Waiting 10s for hive to start...\")\n",
    "        sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa1628b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('test',)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a test table\n",
    "\n",
    "h.execute('CREATE TABLE IF NOT EXISTS default.test (id int, name string) STORED AS ORC')\n",
    "h.execute('SHOW TABLES FROM default')\n",
    "h.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24263d2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=[\"*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 1:12 Table not found 'test':17:16\", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:335', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:199', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:260', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:247', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:541', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:516', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:282', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:563', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:750', \"*org.apache.hadoop.hive.ql.parse.SemanticException:Line 1:12 Table not found 'test':29:13\", 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec:<init>:BaseSemanticAnalyzer.java:1515', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec:<init>:BaseSemanticAnalyzer.java:1427', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2257', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2075', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genResolvedParseTree:SemanticAnalyzer.java:12033', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12129', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:330', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:285', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:659', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1826', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1773', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1768', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:197', '*org.apache.hadoop.hive.ql.metadata.InvalidTableException:Table not found test:32:3', 'org.apache.hadoop.hive.ql.metadata.Hive:getTable:Hive.java:1140', 'org.apache.hadoop.hive.ql.metadata.Hive:getTable:Hive.java:1091', 'org.apache.hadoop.hive.ql.metadata.Hive:getTable:Hive.java:1078', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec:<init>:BaseSemanticAnalyzer.java:1512'], sqlState='42S02', errorCode=10001, errorMessage=\"Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 1:12 Table not found 'test'\"), operationHandle=None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# insert some data into the test table to verify that the ./_data folder is writable\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mINSERT INTO default.test VALUES (1, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJane\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m), (2, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJoe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m h\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSELECT * FROM default.test\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m h\u001b[38;5;241m.\u001b[39mfetchall()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyhive/hive.py:455\u001b[0m, in \u001b[0;36mCursor.execute\u001b[0;34m(self, operation, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(req)\n\u001b[1;32m    454\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mExecuteStatement(req)\n\u001b[0;32m--> 455\u001b[0m \u001b[43m_check_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operationHandle \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39moperationHandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyhive/hive.py:585\u001b[0m, in \u001b[0;36m_check_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    583\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(response)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mstatusCode \u001b[38;5;241m!=\u001b[39m ttypes\u001b[38;5;241m.\u001b[39mTStatusCode\u001b[38;5;241m.\u001b[39mSUCCESS_STATUS:\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OperationalError(response)\n",
      "\u001b[0;31mOperationalError\u001b[0m: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=[\"*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 1:12 Table not found 'test':17:16\", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:335', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:199', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:260', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:247', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:541', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:516', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:282', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:563', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:750', \"*org.apache.hadoop.hive.ql.parse.SemanticException:Line 1:12 Table not found 'test':29:13\", 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec:<init>:BaseSemanticAnalyzer.java:1515', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec:<init>:BaseSemanticAnalyzer.java:1427', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2257', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2075', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genResolvedParseTree:SemanticAnalyzer.java:12033', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12129', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:330', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:285', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:659', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1826', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1773', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1768', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:197', '*org.apache.hadoop.hive.ql.metadata.InvalidTableException:Table not found test:32:3', 'org.apache.hadoop.hive.ql.metadata.Hive:getTable:Hive.java:1140', 'org.apache.hadoop.hive.ql.metadata.Hive:getTable:Hive.java:1091', 'org.apache.hadoop.hive.ql.metadata.Hive:getTable:Hive.java:1078', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec:<init>:BaseSemanticAnalyzer.java:1512'], sqlState='42S02', errorCode=10001, errorMessage=\"Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 1:12 Table not found 'test'\"), operationHandle=None)"
     ]
    }
   ],
   "source": [
    "# insert some data into the test table to verify that the ./_data folder is writable\n",
    "\n",
    "h.execute('INSERT INTO default.test VALUES (1, \"Jane\"), (2, \"Joe\")')\n",
    "h.execute('SELECT * FROM default.test')\n",
    "h.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab740ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the test table and its data\n",
    "\n",
    "h.execute('DROP TABLE default.test')\n",
    "h.execute('SHOW TABLES FROM default')\n",
    "h.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb10d2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('default',), ('iceberg',)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Hive schema for iceberg tables\n",
    "\n",
    "h.execute('CREATE SCHEMA IF NOT EXISTS iceberg')\n",
    "h.execute('SHOW SCHEMAS')\n",
    "h.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150fccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
