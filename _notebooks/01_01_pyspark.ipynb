{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59027d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-unsafe_2.12-3.2.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/03/04 14:02:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# define Spark client\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/usr/local/hadoop/warehouse\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"thrift://hivemetastore:9083\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.cache-enabled\", False) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the table if already exists\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "DROP TABLE IF EXISTS iceberg.default.bank_transfers PURGE\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8383ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the table we will use in further excercises\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS iceberg.default.bank_transfers (\n",
    "    id bigint COMMENT 'transfer id',\n",
    "    amount int COMMENT 'transferred amount, expressed in cents',\n",
    "    transferred_from string COMMENT 'initiator of the transfer',\n",
    "    transferred_to string COMMENT 'receiver of the transfer',\n",
    "    timestamp timestamp COMMENT 'time of the transfer'\n",
    ") PARTITIONED BY (days(timestamp))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70480ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the created table\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "DESCRIBE TABLE EXTENDED iceberg.default.bank_transfers\n",
    "\"\"\").show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert data into the table using SQL\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO iceberg.default.bank_transfers VALUES\n",
    "    (1, 12000, \"ACME INC\",   \"ASTROCORP\",  TIMESTAMP\"2022-11-14T00:55:00\"),\n",
    "    (2, 24000, \"John Doe\",   \"Jane Doe\",   TIMESTAMP\"2022-11-15T02:11:00\"),\n",
    "    (3,   500, \"Deborah S.\", \"Michael C.\", TIMESTAMP\"2022-11-17T16:25:07\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert more data to the table, using the DataFrame API\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(\n",
    "        id=4,\n",
    "        amount=200,\n",
    "        transferred_from=\"CTX Inc.\",\n",
    "        transferred_to=\"XYZ GmbH\",\n",
    "        timestamp=datetime.fromisoformat(\"2022-12-01T07:32:18\")\n",
    "    ),\n",
    "])\n",
    "\n",
    "df.writeTo(\"iceberg.default.bank_transfers\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c20981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the correctness of the inserted data\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM iceberg.default.bank_transfers\n",
    "ORDER BY id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all inserted data and verify that the table is empty\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "DELETE FROM iceberg.default.bank_transfers\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM iceberg.default.bank_transfers\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae406dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2023-03-04 11:24:35.442|2746907607546786975|null               |true               |\n",
      "|2023-03-04 11:32:18.54 |5993786463064479871|2746907607546786975|true               |\n",
      "|2023-03-04 12:14:04.219|6461066575791651155|5993786463064479871|true               |\n",
      "|2023-03-04 12:52:45.43 |3405190307090044624|6461066575791651155|true               |\n",
      "|2023-03-04 12:53:00.982|7378494672306874711|3405190307090044624|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query the history of table commits\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM iceberg.default.bank_transfers.history\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97c28338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|snapshot_id        |manifest_list                                                                                                                |\n",
      "+-------------------+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|2746907607546786975|file:/usr/local/hadoop/warehouse/bank_transfers/metadata/snap-2746907607546786975-1-b56309c7-e352-4a2d-ba40-8fc3e3e82e68.avro|\n",
      "|5993786463064479871|file:/usr/local/hadoop/warehouse/bank_transfers/metadata/snap-5993786463064479871-1-b6a71d61-fd2a-4093-a5d3-821db514303c.avro|\n",
      "|6461066575791651155|file:/usr/local/hadoop/warehouse/bank_transfers/metadata/snap-6461066575791651155-1-76496de8-a61a-4773-af93-a08c847c54fc.avro|\n",
      "|3405190307090044624|file:/usr/local/hadoop/warehouse/bank_transfers/metadata/snap-3405190307090044624-1-87c94d52-1faf-440e-a8e1-c14b9706454a.avro|\n",
      "|7378494672306874711|file:/usr/local/hadoop/warehouse/bank_transfers/metadata/snap-7378494672306874711-1-43c51054-5ce8-4d38-be2b-5cd8668f48d6.avro|\n",
      "+-------------------+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query existing snapshots\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT snapshot_id, manifest_list FROM iceberg.default.bank_transfers.snapshots\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fda65c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "|file                                                                                                             |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "|file:/usr/local/hadoop/warehouse/bank_transfers/metadata/00000-f916bee8-78e6-4dd9-aedf-675833d6f57e.metadata.json|\n",
      "|file:/usr/local/hadoop/warehouse/bank_transfers/metadata/00001-b504896f-ae68-4da2-a6b3-652535a64a3f.metadata.json|\n",
      "|file:/usr/local/hadoop/warehouse/bank_transfers/metadata/00002-0cff973b-9951-4b41-8c4f-2e2049160521.metadata.json|\n",
      "|file:/usr/local/hadoop/warehouse/bank_transfers/metadata/00003-7e6516e8-2cb5-4365-805b-75b603c9309a.metadata.json|\n",
      "|file:/usr/local/hadoop/warehouse/bank_transfers/metadata/00004-f876d760-0aae-410a-a9c3-a2b9aadd49ff.metadata.json|\n",
      "|file:/usr/local/hadoop/warehouse/bank_transfers/metadata/00005-a950b278-c094-4ede-a148-ed036de175b9.metadata.json|\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query metadata log entries\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT file FROM iceberg.default.bank_transfers.metadata_log_entries\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4e364f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
